{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solving Cryptic Crosswords with LLMs: Part 1\n",
        "date created: 04.08.2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycDZN7LR0G5y",
        "outputId": "fdef1139-082c-44a3-c2a1-caf55d5484f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ruhom1kq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-ruhom1kq\n",
            "  Resolved https://github.com/huggingface/transformers to commit 2bd7a27a671fd1d98059124024f580f8f5c0f3b5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.32.0.dev0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.0.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.32.0.dev0)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (3.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.32.0.dev0-py3-none-any.whl size=7449616 sha256=219e08fd96d0ba903a26b6d5ae859afc51153ee5e3ff9a9e97d469ae03f011b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_cg_uj0b/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.32.0.dev0\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx2-PuIszmZF",
        "outputId": "69a92c45-0d4a-4be2-97b1-c13b8bc1accc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.2-py3-none-any.whl (518 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: sentencepiece, xxhash, dill, responses, multiprocess, datasets, evaluate, accelerate\n",
            "Successfully installed accelerate-0.21.0 datasets-2.14.2 dill-0.3.7 evaluate-0.4.0 multiprocess-0.70.15 responses-0.18.0 sentencepiece-0.1.99 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "! pip install torch datasets evaluate accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68lMM09QwnMG"
      },
      "outputs": [],
      "source": [
        "# import modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import json\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# import parameters\n",
        "import parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Extraction and Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "4Y5uKdhYyqEM",
        "outputId": "e0166f5c-2eaf-4387-8df2-9d505dc73fb3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-49388615-9054-4be4-a039-8d3299f35674\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17504</th>\n",
              "      <td>84889</td>\n",
              "      <td>Star man has not completed study (8)</td>\n",
              "      <td>Star</td>\n",
              "      <td>{'text': ['HESPERUS'], 'answer_start': [0]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54485</th>\n",
              "      <td>47857</td>\n",
              "      <td>Revised design in case of emissions prompts an...</td>\n",
              "      <td>anxiety</td>\n",
              "      <td>{'text': ['EDGINESS'], 'answer_start': [0]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18038</th>\n",
              "      <td>148635</td>\n",
              "      <td>Rock and roll primarily lacking in training of...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>{'text': ['GNEISS'], 'answer_start': [0]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27748</th>\n",
              "      <td>160715</td>\n",
              "      <td>Previously in unison (2,3,4)</td>\n",
              "      <td>Previously/in unison</td>\n",
              "      <td>{'text': ['AT ONE TIME'], 'answer_start': [0]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56655</th>\n",
              "      <td>560759</td>\n",
              "      <td>New drapes due to be brought round (9)</td>\n",
              "      <td>brought round</td>\n",
              "      <td>{'text': ['PERSUADED'], 'answer_start': [0]}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49388615-9054-4be4-a039-8d3299f35674')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-91ecd9a8-0d30-424f-a65d-a483c2422e80\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-91ecd9a8-0d30-424f-a65d-a483c2422e80')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-91ecd9a8-0d30-424f-a65d-a483c2422e80 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49388615-9054-4be4-a039-8d3299f35674 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49388615-9054-4be4-a039-8d3299f35674');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           id                                           question  \\\n",
              "17504   84889               Star man has not completed study (8)   \n",
              "54485   47857  Revised design in case of emissions prompts an...   \n",
              "18038  148635  Rock and roll primarily lacking in training of...   \n",
              "27748  160715                       Previously in unison (2,3,4)   \n",
              "56655  560759             New drapes due to be brought round (9)   \n",
              "\n",
              "                    context                                         answers  \n",
              "17504                  Star     {'text': ['HESPERUS'], 'answer_start': [0]}  \n",
              "54485               anxiety     {'text': ['EDGINESS'], 'answer_start': [0]}  \n",
              "18038                  Rock       {'text': ['GNEISS'], 'answer_start': [0]}  \n",
              "27748  Previously/in unison  {'text': ['AT ONE TIME'], 'answer_start': [0]}  \n",
              "56655         brought round    {'text': ['PERSUADED'], 'answer_start': [0]}  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import data\n",
        "clues_raw = pd.read_csv(clues_path_raw).dropna().sample(frac=1)\n",
        "clues = clues_raw.copy()[['rowid', 'clue', 'answer', 'definition']]\n",
        "\n",
        "# Transform columns into the format required by the trainer module\n",
        "clues['rowid'] = clues['rowid'].astype(str)\n",
        "clues['question'] = clues['clue']\n",
        "clues['context'] = clues['definition'] \n",
        "clues['answers'] = clues['answer'].map(lambda x : {\"text\" : [x], \"answer_start\" : [0]})\n",
        "clues['answers'].apply(lambda x : ast.literal_eval(str(x)))\n",
        "clues = clues.rename(columns={'rowid' : 'id'})\n",
        "clues = clues[['id', 'question', 'context', 'answers']].dropna()\n",
        "\n",
        "# Print examples\n",
        "clues.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYXC8Ccvysda"
      },
      "outputs": [],
      "source": [
        "# Split data into train, validation, test\n",
        "\n",
        "train_val = clues.sample(frac=0.9,random_state=200)\n",
        "test = clues.drop(train_val.index)\n",
        "train = train_val.sample(frac=0.9,random_state=200)\n",
        "validation = train_val.drop(train.index)\n",
        "\n",
        "# Save data\n",
        "clues.to_csv(clues_path_processed, index=False)\n",
        "train.to_csv(clues_path_train, index=False)\n",
        "validation.to_csv(clues_path_validation, index=False)\n",
        "test.to_csv(clues_path_test, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N.B. the modelling parameters are in the parameter.py file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtuUqMLby-7t",
        "outputId": "105b6243-8889-4328-f774-b7a0dfad3dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-02 20:10:52.849663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/02/2023 20:10:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/02/2023 20:10:55 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./tmp/debug_seq2seq_xword_v3/runs/Aug02_20-10-55_b02fb346877f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=./tmp/debug_seq2seq_xword_v3/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=192,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./tmp/debug_seq2seq_xword_v3/,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=None' instead.\n",
            "  warnings.warn(\n",
            "Using custom data configuration default-96ecd430a57eef5d\n",
            "08/02/2023 20:10:56 - INFO - datasets.builder - Using custom data configuration default-96ecd430a57eef5d\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
            "08/02/2023 20:10:56 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
            "Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "08/02/2023 20:10:56 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n",
            "08/02/2023 20:10:56 - INFO - datasets.builder - Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 21326.97it/s]\n",
            "Downloading took 0.0 min\n",
            "08/02/2023 20:10:56 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "08/02/2023 20:10:56 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2074.68it/s]\n",
            "Generating train split\n",
            "08/02/2023 20:10:56 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 81000 examples [00:00, 350660.65 examples/s]\n",
            "Generating validation split\n",
            "08/02/2023 20:10:56 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 9000 examples [00:00, 372059.02 examples/s]\n",
            "Generating test split\n",
            "08/02/2023 20:10:56 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 10000 examples [00:00, 380539.29 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "08/02/2023 20:10:57 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n",
            "08/02/2023 20:10:57 - INFO - datasets.builder - Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n",
            "Downloading (…)lve/main/config.json: 100% 537/537 [00:00<00:00, 3.54MB/s]\n",
            "[INFO|configuration_utils.py:715] 2023-08-02 20:10:57,591 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-08-02 20:10:57,594 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-small-ssm\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 1.84k/1.84k [00:00<00:00, 11.9MB/s]\n",
            "[INFO|configuration_utils.py:715] 2023-08-02 20:10:58,051 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-08-02 20:10:58,051 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-small-ssm\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 1.26MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 1.79k/1.79k [00:00<00:00, 9.48MB/s]\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-08-02 20:11:00,077 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-08-02 20:11:00,077 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-08-02 20:11:00,077 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-08-02 20:11:00,077 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-08-02 20:11:00,077 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-08-02 20:11:00,077 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-08-02 20:11:00,078 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-small-ssm\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:305] 2023-08-02 20:11:00,078 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565, and set the legacy attribute accordingly.\n",
            "[INFO|configuration_utils.py:715] 2023-08-02 20:11:00,127 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-08-02 20:11:00,127 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-small-ssm\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 308M/308M [00:18<00:00, 16.3MB/s]\n",
            "[INFO|modeling_utils.py:2638] 2023-08-02 20:11:20,841 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:616] 2023-08-02 20:11:21,053 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.32.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3373] 2023-08-02 20:11:22,140 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:3381] 2023-08-02 20:11:22,140 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/t5-small-ssm.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Downloading (…)neration_config.json: 100% 147/147 [00:00<00:00, 911kB/s]\n",
            "[INFO|configuration_utils.py:578] 2023-08-02 20:11:22,597 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--t5-small-ssm/snapshots/9d5a5cda37e65b1b34679f088e2e61b070348339/generation_config.json\n",
            "[INFO|configuration_utils.py:616] 2023-08-02 20:11:22,598 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.32.0.dev0\"\n",
            "}\n",
            "\n",
            "Running tokenizer on train dataset:   0% 0/81000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-f2006137a3eadd40.arrow\n",
            "08/02/2023 20:11:22 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-f2006137a3eadd40.arrow\n",
            "Running tokenizer on train dataset: 100% 81000/81000 [00:13<00:00, 5879.68 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/9000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-e8cc1d301bd7d9c1.arrow\n",
            "08/02/2023 20:11:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-e8cc1d301bd7d9c1.arrow\n",
            "Running tokenizer on validation dataset: 100% 9000/9000 [00:01<00:00, 4972.50 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/10000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-28f0652c8c70e309.arrow\n",
            "08/02/2023 20:11:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-96ecd430a57eef5d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-28f0652c8c70e309.arrow\n",
            "Running tokenizer on prediction dataset: 100% 10000/10000 [00:02<00:00, 4971.70 examples/s]\n",
            "Downloading builder script: 100% 6.47k/6.47k [00:00<00:00, 21.1MB/s]\n",
            "Downloading extra modules: 100% 11.3k/11.3k [00:00<00:00, 40.7MB/s]\n",
            "[INFO|trainer.py:1684] 2023-08-02 20:11:47,474 >> ***** Running training *****\n",
            "[INFO|trainer.py:1685] 2023-08-02 20:11:47,474 >>   Num examples = 81,000\n",
            "[INFO|trainer.py:1686] 2023-08-02 20:11:47,474 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1687] 2023-08-02 20:11:47,474 >>   Instantaneous batch size per device = 192\n",
            "[INFO|trainer.py:1690] 2023-08-02 20:11:47,475 >>   Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "[INFO|trainer.py:1691] 2023-08-02 20:11:47,475 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1692] 2023-08-02 20:11:47,475 >>   Total optimization steps = 10,550\n",
            "[INFO|trainer.py:1693] 2023-08-02 20:11:47,475 >>   Number of trainable parameters = 76,961,152\n",
            "  0% 0/10550 [00:00<?, ?it/s][WARNING|logging.py:290] 2023-08-02 20:11:47,506 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 3.3392, 'learning_rate': 0.0019052132701421801, 'epoch': 1.18}\n",
            "  5% 500/10550 [09:19<3:06:16,  1.11s/it][INFO|trainer.py:2812] 2023-08-02 20:21:07,411 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 20:21:07,412 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 20:21:07,413 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 20:21:13,006 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 20:21:13,007 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 20:21:13,007 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 20:21:13,045 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-500/spiece.model\n",
            "{'loss': 2.6732, 'learning_rate': 0.0018104265402843602, 'epoch': 2.37}\n",
            "  9% 1000/10550 [18:46<2:57:24,  1.11s/it][INFO|trainer.py:2812] 2023-08-02 20:30:34,413 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-1000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 20:30:34,414 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 20:30:34,415 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 20:30:35,354 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 20:30:35,355 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 20:30:35,355 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 20:30:35,390 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-1000/spiece.model\n",
            "{'loss': 2.3167, 'learning_rate': 0.0017156398104265403, 'epoch': 3.55}\n",
            " 14% 1500/10550 [28:08<2:48:17,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 20:39:56,314 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-1500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 20:39:56,315 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 20:39:56,315 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 20:40:00,264 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 20:40:00,266 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 20:40:00,267 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 20:40:00,303 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-1500/spiece.model\n",
            "{'loss': 2.0413, 'learning_rate': 0.0016208530805687205, 'epoch': 4.74}\n",
            " 19% 2000/10550 [37:34<2:39:13,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 20:49:21,980 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-2000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 20:49:21,981 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 20:49:21,981 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 20:49:28,024 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 20:49:28,025 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 20:49:28,025 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 20:49:28,059 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-2000/spiece.model\n",
            "{'loss': 1.8107, 'learning_rate': 0.0015260663507109004, 'epoch': 5.92}\n",
            " 24% 2500/10550 [47:04<2:29:01,  1.11s/it][INFO|trainer.py:2812] 2023-08-02 20:58:52,321 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-2500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 20:58:52,322 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 20:58:52,322 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 20:58:57,946 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 20:58:57,947 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 20:58:57,948 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 20:58:58,004 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-2500/spiece.model\n",
            "{'loss': 1.595, 'learning_rate': 0.0014312796208530805, 'epoch': 7.11}\n",
            " 28% 3000/10550 [56:31<2:20:03,  1.11s/it][INFO|trainer.py:2812] 2023-08-02 21:08:19,280 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-3000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 21:08:19,282 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 21:08:19,282 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 21:08:25,166 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 21:08:25,166 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 21:08:25,167 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 21:08:25,199 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-3000/spiece.model\n",
            "{'loss': 1.4172, 'learning_rate': 0.0013364928909952607, 'epoch': 8.29}\n",
            " 33% 3500/10550 [1:05:58<2:10:52,  1.11s/it][INFO|trainer.py:2812] 2023-08-02 21:17:46,303 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-3500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 21:17:46,304 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 21:17:46,304 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 21:17:51,911 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 21:17:51,912 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 21:17:51,913 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 21:17:51,948 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-3500/spiece.model\n",
            "{'loss': 1.2687, 'learning_rate': 0.0012417061611374408, 'epoch': 9.48}\n",
            " 38% 4000/10550 [1:15:25<2:01:48,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 21:27:13,026 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-4000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 21:27:13,027 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 21:27:13,028 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 21:27:18,754 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 21:27:18,755 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 21:27:18,755 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 21:27:18,792 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-4000/spiece.model\n",
            "{'loss': 1.1469, 'learning_rate': 0.0011469194312796209, 'epoch': 10.66}\n",
            " 43% 4500/10550 [1:24:52<1:52:03,  1.11s/it][INFO|trainer.py:2812] 2023-08-02 21:36:40,429 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-4500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 21:36:40,430 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 21:36:40,430 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 21:36:41,321 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 21:36:41,326 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 21:36:41,328 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 21:36:41,392 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-4500/spiece.model\n",
            "{'loss': 1.0355, 'learning_rate': 0.001052132701421801, 'epoch': 11.85}\n",
            " 47% 5000/10550 [1:34:25<1:43:38,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 21:46:13,010 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-5000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 21:46:13,011 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 21:46:13,011 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 21:46:15,121 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 21:46:15,122 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 21:46:15,123 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 21:46:15,161 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-5000/spiece.model\n",
            "{'loss': 0.9292, 'learning_rate': 0.000957345971563981, 'epoch': 13.03}\n",
            " 52% 5500/10550 [1:43:48<1:34:03,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 21:55:36,358 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-5500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 21:55:36,359 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 21:55:36,360 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 21:55:42,180 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 21:55:42,181 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 21:55:42,181 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 21:55:42,238 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-5500/spiece.model\n",
            "{'loss': 0.8227, 'learning_rate': 0.0008625592417061612, 'epoch': 14.22}\n",
            " 57% 6000/10550 [1:53:16<1:24:49,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 22:05:03,847 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-6000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 22:05:03,848 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 22:05:03,849 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 22:05:10,189 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 22:05:10,190 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 22:05:10,190 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 22:05:10,228 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-6000/spiece.model\n",
            "{'loss': 0.7479, 'learning_rate': 0.0007677725118483413, 'epoch': 15.4}\n",
            " 62% 6500/10550 [2:02:44<1:15:44,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 22:14:32,252 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-6500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 22:14:32,254 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 22:14:32,254 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 22:14:38,473 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 22:14:38,480 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 22:14:38,480 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-6500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 22:14:38,545 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-6500/spiece.model\n",
            "{'loss': 0.6788, 'learning_rate': 0.0006729857819905212, 'epoch': 16.59}\n",
            " 66% 7000/10550 [2:12:12<1:06:12,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 22:24:00,021 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-7000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 22:24:00,022 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 22:24:00,023 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 22:24:01,017 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 22:24:01,018 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 22:24:01,019 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 22:24:01,084 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-7000/spiece.model\n",
            "{'loss': 0.6152, 'learning_rate': 0.0005781990521327014, 'epoch': 17.77}\n",
            " 71% 7500/10550 [2:21:35<57:02,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 22:33:22,801 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-7500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 22:33:22,802 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 22:33:22,802 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 22:33:28,811 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 22:33:28,812 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 22:33:28,813 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-7500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 22:33:28,848 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-7500/spiece.model\n",
            "{'loss': 0.5631, 'learning_rate': 0.00048341232227488154, 'epoch': 18.96}\n",
            " 76% 8000/10550 [2:31:06<47:22,  1.11s/it][INFO|trainer.py:2812] 2023-08-02 22:42:53,846 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-8000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 22:42:53,847 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 22:42:53,847 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 22:42:59,663 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 22:42:59,664 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 22:42:59,664 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 22:42:59,719 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-8000/spiece.model\n",
            "{'loss': 0.5054, 'learning_rate': 0.0003886255924170616, 'epoch': 20.14}\n",
            " 81% 8500/10550 [2:40:37<38:12,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 22:52:25,145 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-8500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 22:52:25,146 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 22:52:25,147 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 22:52:31,082 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 22:52:31,084 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 22:52:31,084 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-8500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 22:52:31,118 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-8500/spiece.model\n",
            "{'loss': 0.4647, 'learning_rate': 0.0002938388625592417, 'epoch': 21.33}\n",
            " 85% 9000/10550 [2:50:05<28:58,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 23:01:52,866 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-9000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 23:01:52,867 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 23:01:52,868 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 23:01:58,891 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 23:01:58,892 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 23:01:58,893 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 23:01:58,931 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-9000/spiece.model\n",
            "{'loss': 0.4303, 'learning_rate': 0.0001990521327014218, 'epoch': 22.51}\n",
            " 90% 9500/10550 [2:59:38<19:33,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 23:11:26,139 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-9500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 23:11:26,140 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 23:11:26,140 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 23:11:26,974 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 23:11:26,975 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 23:11:26,976 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-9500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 23:11:27,014 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-9500/spiece.model\n",
            "{'loss': 0.4036, 'learning_rate': 0.00010426540284360189, 'epoch': 23.7}\n",
            " 95% 10000/10550 [3:09:01<10:14,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 23:20:48,737 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-10000\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 23:20:48,738 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10000/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 23:20:48,738 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10000/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 23:20:54,349 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 23:20:54,351 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 23:20:54,351 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 23:20:54,424 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-10000/spiece.model\n",
            "{'loss': 0.3845, 'learning_rate': 9.478672985781992e-06, 'epoch': 24.88}\n",
            "100% 10500/10550 [3:18:29<00:56,  1.12s/it][INFO|trainer.py:2812] 2023-08-02 23:30:16,813 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/checkpoint-10500\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 23:30:16,814 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10500/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 23:30:16,815 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10500/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 23:30:22,331 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 23:30:22,332 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 23:30:22,332 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/checkpoint-10500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 23:30:22,392 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/checkpoint-10500/spiece.model\n",
            "100% 10550/10550 [3:19:37<00:00,  1.07s/it][INFO|trainer.py:1932] 2023-08-02 23:31:24,905 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 11977.4547, 'train_samples_per_second': 169.068, 'train_steps_per_second': 0.881, 'train_loss': 1.195629701840369, 'epoch': 25.0}\n",
            "100% 10550/10550 [3:19:37<00:00,  1.14s/it]\n",
            "[INFO|trainer.py:2812] 2023-08-02 23:31:24,932 >> Saving model checkpoint to ./tmp/debug_seq2seq_xword_v3/\n",
            "[INFO|configuration_utils.py:460] 2023-08-02 23:31:24,933 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/config.json\n",
            "[INFO|configuration_utils.py:392] 2023-08-02 23:31:24,934 >> Configuration saved in ./tmp/debug_seq2seq_xword_v3/generation_config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-08-02 23:31:25,910 >> Model weights saved in ./tmp/debug_seq2seq_xword_v3/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-08-02 23:31:25,911 >> tokenizer config file saved in ./tmp/debug_seq2seq_xword_v3/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-08-02 23:31:25,912 >> Special tokens file saved in ./tmp/debug_seq2seq_xword_v3/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-08-02 23:31:25,992 >> Copy vocab file to ./tmp/debug_seq2seq_xword_v3/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     1.1956\n",
            "  train_runtime            = 3:19:37.45\n",
            "  train_samples            =      81000\n",
            "  train_samples_per_second =    169.068\n",
            "  train_steps_per_second   =      0.881\n",
            "08/02/2023 23:31:26 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:752] 2023-08-02 23:31:26,007 >> The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3086] 2023-08-02 23:31:26,020 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3088] 2023-08-02 23:31:26,020 >>   Num examples = 9000\n",
            "[INFO|trainer.py:3091] 2023-08-02 23:31:26,020 >>   Batch size = 8\n",
            "[INFO|configuration_utils.py:616] 2023-08-02 23:31:26,031 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.32.0.dev0\"\n",
            "}\n",
            "\n",
            "100% 1125/1125 [02:28<00:00,  7.59it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_HasAns_exact       =    10.9111\n",
            "  eval_HasAns_f1          =    11.9999\n",
            "  eval_HasAns_total       =       9000\n",
            "  eval_best_exact         =    10.9111\n",
            "  eval_best_exact_thresh  =        0.0\n",
            "  eval_best_f1            =    11.9999\n",
            "  eval_best_f1_thresh     =        0.0\n",
            "  eval_exact              =    10.9111\n",
            "  eval_f1                 =    11.9999\n",
            "  eval_loss               =      3.001\n",
            "  eval_runtime            = 0:02:25.39\n",
            "  eval_samples            =       9000\n",
            "  eval_samples_per_second =       61.9\n",
            "  eval_steps_per_second   =      7.738\n",
            "  eval_total              =       9000\n",
            "08/02/2023 23:33:56 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:752] 2023-08-02 23:33:56,645 >> The following columns in the test set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3086] 2023-08-02 23:33:56,646 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3088] 2023-08-02 23:33:56,647 >>   Num examples = 10000\n",
            "[INFO|trainer.py:3091] 2023-08-02 23:33:56,647 >>   Batch size = 8\n",
            "100% 1249/1250 [02:34<00:00,  9.21it/s]***** predict metrics *****\n",
            "  predict_samples         =      10000\n",
            "  test_HasAns_exact       =      10.61\n",
            "  test_HasAns_f1          =    11.5465\n",
            "  test_HasAns_total       =      10000\n",
            "  test_best_exact         =      10.61\n",
            "  test_best_exact_thresh  =        0.0\n",
            "  test_best_f1            =    11.5465\n",
            "  test_best_f1_thresh     =        0.0\n",
            "  test_exact              =      10.61\n",
            "  test_f1                 =    11.5465\n",
            "  test_loss               =     3.0438\n",
            "  test_runtime            = 0:02:35.12\n",
            "  test_samples_per_second =     64.465\n",
            "  test_steps_per_second   =      8.058\n",
            "  test_total              =      10000\n",
            "100% 1250/1250 [02:41<00:00,  7.75it/s]\n"
          ]
        }
      ],
      "source": [
        "! python run_seq2seq_qa.py \\\n",
        "  --model_name_or_path model_name_t5 \\\n",
        "  --train_file clues_path_train \\\n",
        "  --validation_file clues_path_validation \\\n",
        "  --test_file clues_path_test \\\n",
        "  --question_column question \\\n",
        "  --context_column context \\\n",
        "  --answer_column answers \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_pred \\\n",
        "  --predict_with_generate \\\n",
        "  --version_2_with_negative \\\n",
        "  --per_device_train_batch_size batch_size \\\n",
        "  --learning_rate lr \\\n",
        "  --num_train_epochs num_epochs \\\n",
        "  --max_seq_length max_seq_length \\\n",
        "  --overwrite_output_dir overwrite_dir \\\n",
        "  --output_dir output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlf5qvnalyKI"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Q5MPcACMzWMc",
        "outputId": "1fbf4461-1f8d-4451-8489-a474ca75294b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-071de08d-271b-48eb-8ca4-0dc0eadd8125\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rowid</th>\n",
              "      <th>prediction_text</th>\n",
              "      <th>no_answer_probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>160715</td>\n",
              "      <td>IN THE MAIL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>501666</td>\n",
              "      <td>MARTINI</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>488088</td>\n",
              "      <td>PORT A CAKE</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>135879</td>\n",
              "      <td>ANIMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>392986</td>\n",
              "      <td>PRIORUM</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-071de08d-271b-48eb-8ca4-0dc0eadd8125')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-733f9659-179d-4cdf-b268-eae7c0d94bc4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-733f9659-179d-4cdf-b268-eae7c0d94bc4')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-733f9659-179d-4cdf-b268-eae7c0d94bc4 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-071de08d-271b-48eb-8ca4-0dc0eadd8125 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-071de08d-271b-48eb-8ca4-0dc0eadd8125');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    rowid prediction_text  no_answer_probability\n",
              "0  160715     IN THE MAIL                      0\n",
              "1  501666         MARTINI                      0\n",
              "2  488088     PORT A CAKE                      0\n",
              "3  135879          ANIMAL                      0\n",
              "4  392986         PRIORUM                      0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read prediction output file\n",
        "predictions = pd.read_json(predictions_path)\n",
        "predictions = predictions.rename(columns={'id' : 'rowid'})\n",
        "\n",
        "# Show some examples\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "hl5mQOMv9JMM",
        "outputId": "fe953526-8c45-44c5-aec8-4a7c75d1efc6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-29d5cb82-812d-4b90-a081-012e62b26dd9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clue</th>\n",
              "      <th>definition</th>\n",
              "      <th>answer</th>\n",
              "      <th>prediction_text</th>\n",
              "      <th>correct_len</th>\n",
              "      <th>correct</th>\n",
              "      <th>correct_len_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Previously in unison (2,3,4)</td>\n",
              "      <td>Previously/in unison</td>\n",
              "      <td>AT ONE TIME</td>\n",
              "      <td>IN THE MAIL</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Massachusetts Republican: “Can I drink?” (7)</td>\n",
              "      <td>drink</td>\n",
              "      <td>MARTINI</td>\n",
              "      <td>MARTINI</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Czar due to travel round holiday region (4,1’4)</td>\n",
              "      <td>holiday region</td>\n",
              "      <td>COTE D</td>\n",
              "      <td>PORT A CAKE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Awful fear that is brought by magical creature...</td>\n",
              "      <td>magical creature no longer</td>\n",
              "      <td>FAERIE</td>\n",
              "      <td>ANIMAL</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Highest rising American with power over Britis...</td>\n",
              "      <td>Highest</td>\n",
              "      <td>T</td>\n",
              "      <td>PRIORUM</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29d5cb82-812d-4b90-a081-012e62b26dd9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-13bfc213-38bc-42d4-9b38-a76e7336aa70\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13bfc213-38bc-42d4-9b38-a76e7336aa70')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-13bfc213-38bc-42d4-9b38-a76e7336aa70 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29d5cb82-812d-4b90-a081-012e62b26dd9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29d5cb82-812d-4b90-a081-012e62b26dd9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                clue  \\\n",
              "0                       Previously in unison (2,3,4)   \n",
              "1       Massachusetts Republican: “Can I drink?” (7)   \n",
              "2    Czar due to travel round holiday region (4,1’4)   \n",
              "3  Awful fear that is brought by magical creature...   \n",
              "4  Highest rising American with power over Britis...   \n",
              "\n",
              "                   definition       answer prediction_text  correct_len  \\\n",
              "0        Previously/in unison  AT ONE TIME     IN THE MAIL            1   \n",
              "1                       drink      MARTINI         MARTINI            1   \n",
              "2              holiday region       COTE D     PORT A CAKE            0   \n",
              "3  magical creature no longer       FAERIE          ANIMAL            1   \n",
              "4                     Highest            T         PRIORUM            0   \n",
              "\n",
              "   correct  correct_len_1  \n",
              "0        0              1  \n",
              "1        1              1  \n",
              "2        0              0  \n",
              "3        0              1  \n",
              "4        0              0  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Join and compare with clues dataset to see correct / incorrect answers\n",
        "\n",
        "compare = clues_raw.merge(predictions, on='rowid')[['clue', 'definition', 'answer', 'prediction_text']]\n",
        "compare['correct_len'] = np.where(compare['prediction_text'].str.len() == compare['answer'].str.len(), 1, 0)\n",
        "compare['correct'] = np.where(compare['prediction_text'] == compare['answer'], 1, 0)\n",
        "compare['correct_len_1'] = np.where(abs(compare['prediction_text'].str.len() - compare['answer'].str.len()) <=1, 1, 0)\n",
        "\n",
        "# Get stats of correct vs incorrect cols\n",
        "compare.groupby(['correct', 'correct_len', 'correct_len_1']).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fU3f48SNqMHg",
        "outputId": "fa03fb03-a42d-4b02-d121-08dee8855ecf"
      },
      "source": [
        "## Plot loss vs epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f'{output_dir}/trainer_state.json', 'rb') as f:\n",
        "    tr = json.load(f)\n",
        "\n",
        "epoch_list = [0]\n",
        "loss_list = [None]\n",
        "learning_rate_list = [lr]\n",
        "\n",
        "# Collect the list of each metric\n",
        "for x in tr['log_history'][:-1]:\n",
        "    epoch_list.append(x['epoch'])\n",
        "    loss_list.append(x['loss'])\n",
        "    learning_rate_list.append(x['learning_rate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(dict(\n",
        "    epoch = epoch_list,\n",
        "    loss = loss_list,\n",
        "    learning_rate = learning_rate_list\n",
        "))\n",
        "\n",
        "\n",
        "# Create figure with secondary y-axis\n",
        "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "# Create traces\n",
        "fig.add_trace(go.Scatter(x=epoch_list, y=loss_list,\n",
        "                    mode='lines',\n",
        "                    name='loss'))\n",
        "fig.add_trace(go.Scatter(x=epoch_list, y=learning_rate_list, \n",
        "                    mode='lines+markers',\n",
        "                    name='learning_rate'), secondary_y=True)\n",
        "\n",
        "# Set x-axis title\n",
        "fig.update_xaxes(title_text=\"Epoch\")\n",
        "\n",
        "# Set y-axes titles\n",
        "fig.update_yaxes(title_text=\"Loss\", secondary_y=False)\n",
        "fig.update_yaxes(title_text=\"Learning Rate\", secondary_y=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
